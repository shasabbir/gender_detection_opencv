{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a34c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2baf009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial parameters\n",
    "epochs = 20\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "img_dims = (96,96,3)\n",
    "\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f10f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image files from the dataset\n",
    "image_files = [f for f in glob.glob(r'D:\\task3\\Gender-Detection\\gender_dataset_face' + \"/**/*\", recursive=True) if not os.path.isdir(f)]\n",
    "random.shuffle(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8580288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting images to arrays and labelling the categories\n",
    "for img in image_files:\n",
    "\n",
    "    image = cv2.imread(img)\n",
    "    \n",
    "    image = cv2.resize(image, (img_dims[0],img_dims[1]))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "\n",
    "    label = img.split(os.path.sep)[-2] # C:\\Files\\gender_dataset_face\\woman\\face_1162.jpg\n",
    "    if label == \"woman\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "        \n",
    "    labels.append([label]) # [[1], [0], [0], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52393dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da582a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset for training and validation\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "\n",
    "trainY = to_categorical(trainY, num_classes=2) # [[1, 0], [0, 1], [0, 1], ...]\n",
    "testY = to_categorical(testY, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481bbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmenting datset \n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb6dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def build(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\": #Returns a string, either 'channels_first' or 'channels_last'\n",
    "        inputShape = (depth, height, width)\n",
    "        chanDim = 1\n",
    "    \n",
    "    # The axis that should be normalized, after a Conv2D layer with data_format=\"channels_first\", \n",
    "    # set axis=1 in BatchNormalization.\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb07797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabbir\\.conda\\envs\\cvpr\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - 55s 2s/step - loss: 0.9287 - accuracy: 0.6821 - val_loss: 0.6711 - val_accuracy: 0.5584\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 50s 2s/step - loss: 0.4858 - accuracy: 0.8111 - val_loss: 1.3273 - val_accuracy: 0.5281\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.4252 - accuracy: 0.8428 - val_loss: 0.8925 - val_accuracy: 0.5281\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 50s 2s/step - loss: 0.3613 - accuracy: 0.8575 - val_loss: 0.7987 - val_accuracy: 0.5628\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 51s 2s/step - loss: 0.3047 - accuracy: 0.8689 - val_loss: 0.8079 - val_accuracy: 0.5433\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 51s 2s/step - loss: 0.3575 - accuracy: 0.8644 - val_loss: 0.8147 - val_accuracy: 0.5303\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.2672 - accuracy: 0.8973 - val_loss: 1.1911 - val_accuracy: 0.5368\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 52s 2s/step - loss: 0.2629 - accuracy: 0.9101 - val_loss: 1.1416 - val_accuracy: 0.5281\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 51s 2s/step - loss: 0.2409 - accuracy: 0.9107 - val_loss: 2.1521 - val_accuracy: 0.5281\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 49s 2s/step - loss: 0.2004 - accuracy: 0.9280 - val_loss: 0.7414 - val_accuracy: 0.5801\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 49s 2s/step - loss: 0.2284 - accuracy: 0.9118 - val_loss: 1.2268 - val_accuracy: 0.5541\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 53s 2s/step - loss: 0.2030 - accuracy: 0.9183 - val_loss: 0.6492 - val_accuracy: 0.7316\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 51s 2s/step - loss: 0.1957 - accuracy: 0.9329 - val_loss: 1.0515 - val_accuracy: 0.5996\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.2051 - accuracy: 0.9242 - val_loss: 1.1346 - val_accuracy: 0.6558\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 50s 2s/step - loss: 0.1526 - accuracy: 0.9340 - val_loss: 0.3214 - val_accuracy: 0.8745\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 52s 2s/step - loss: 0.1240 - accuracy: 0.9526 - val_loss: 0.3092 - val_accuracy: 0.8680\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 50s 2s/step - loss: 0.1586 - accuracy: 0.9358 - val_loss: 0.9869 - val_accuracy: 0.6926\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 49s 2s/step - loss: 0.1290 - accuracy: 0.9560 - val_loss: 0.2815 - val_accuracy: 0.8939\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 52s 2s/step - loss: 0.1392 - accuracy: 0.9547 - val_loss: 0.1529 - val_accuracy: 0.9459\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 52s 2s/step - loss: 0.1295 - accuracy: 0.9528 - val_loss: 0.4764 - val_accuracy: 0.8506\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = build(width=img_dims[0], height=img_dims[1], depth=img_dims[2],\n",
    "                            classes=2)\n",
    "\n",
    "# compile the model\n",
    "opt = Adam(lr=lr, decay=lr/epochs)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=batch_size),\n",
    "                        validation_data=(testX,testY),\n",
    "                        steps_per_epoch=len(trainX) // batch_size,\n",
    "                        epochs=epochs, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a779c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gender_detection.model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('gender_detection.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cc8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
